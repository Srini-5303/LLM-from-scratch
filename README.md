bigram.py - source code.
input.txt - data corpus

Built a language model of 0.21M parameters from scratch, similar to the Transformer architecture, to generate text in Shakespearean language.
Implemented core components such as multi-head attention, feedforward neural networks, and positional embeddings using PyTorch.
Utilized techniques like masked self-attention and layer normalization to improve model performance and Designed a training loop with 5000 iterations, cross-entropy loss, dropout regularization, and AdamW optimization to enhance the model efficiency.
Generated text sequences by conditioning on input tokens, using a sampling approach for next token prediction.

bigram.py - source code;
input.txt - data corpus

1) Built a language model of 0.21M parameters from scratch, similar to the Transformer architecture, to generate text in Shakespearean language.
2) Implemented core components such as multi-head attention, feedforward neural networks, and positional embeddings using PyTorch.
3) Utilized techniques like masked self-attention and layer normalization to improve model performance and Designed a training loop with 5000 iterations, cross-entropy loss, dropout regularization, and AdamW optimization to enhance the model efficiency.
4) Generated text sequences by conditioning on input tokens, using a sampling approach for next token prediction.
